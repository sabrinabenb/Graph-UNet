{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabrinabenb/Graph-UNet/blob/main/Graph_Unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwH7IehpL4i8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['PYTHONWARNINGS'] = \"ignore\"\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, List, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch_sparse import spspmm\n",
        "\n",
        "from torch_geometric.typing import OptTensor, PairTensor\n",
        "from torch_geometric.utils import (\n",
        "    add_self_loops,\n",
        "    remove_self_loops,\n",
        "    sort_edge_index,\n",
        ")\n",
        "from torch_geometric.utils.repeat import repeat\n",
        "%matplotlib inline\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "def visualize_graph(G, color):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    nx.draw_networkx(G, pos=nx.spring_layout(G, seed=42), with_labels=False,\n",
        "                     node_color=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "22ZAYYi7WemX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
        "\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]\n",
        "g = dataset[0] # Get the first graph object.\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GvwRixFWy9v",
        "outputId": "96b9d91a-bab9-4e17-80a6-b5a0eb1dc66c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Cora():\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 1433\n",
            "Number of classes: 7\n",
            "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, List, Union\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "from torch_geometric.nn import GCNConv, TopKPooling\n",
        "from torch_geometric.nn.resolver import activation_resolver\n",
        "from torch_geometric.typing import OptTensor, PairTensor\n",
        "from torch_geometric.utils import (\n",
        "    add_self_loops,\n",
        "    remove_self_loops,\n",
        "    to_torch_csr_tensor,\n",
        ")\n",
        "from torch_geometric.utils.repeat import repeat\n",
        "\n",
        "\n",
        "class GraphUNet(torch.nn.Module):\n",
        "    r\"\"\"The Graph U-Net model from the `\"Graph U-Nets\"\n",
        "    <https://arxiv.org/abs/1905.05178>`_ paper which implements a U-Net like\n",
        "    architecture with graph pooling and unpooling operations.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each input sample.\n",
        "        hidden_channels (int): Size of each hidden sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        depth (int): The depth of the U-Net architecture.\n",
        "        pool_ratios (float or [float], optional): Graph pooling ratio for each\n",
        "            depth. (default: :obj:`0.5`)\n",
        "        sum_res (bool, optional): If set to :obj:`False`, will use\n",
        "            concatenation for integration of skip connections instead\n",
        "            summation. (default: :obj:`True`)\n",
        "        act (torch.nn.functional, optional): The nonlinearity to use.\n",
        "            (default: :obj:`torch.nn.functional.relu`)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        depth: int,\n",
        "        pool_ratios: Union[float, List[float]] = 0.5,\n",
        "        sum_res: bool = True,\n",
        "        act: Union[str, Callable] = 'relu',\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert depth >= 1\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.depth = depth\n",
        "        self.pool_ratios = repeat(pool_ratios, depth)\n",
        "        self.act = activation_resolver(act)\n",
        "        self.sum_res = sum_res\n",
        "\n",
        "        channels = hidden_channels\n",
        "\n",
        "        self.down_convs = torch.nn.ModuleList()\n",
        "        self.pools = torch.nn.ModuleList()\n",
        "        self.down_convs.append(GCNConv(in_channels, channels, improved=True))\n",
        "        for i in range(depth):\n",
        "            self.pools.append(TopKPooling(channels, self.pool_ratios[i]))\n",
        "            self.down_convs.append(GCNConv(channels, channels, improved=True))\n",
        "\n",
        "        in_channels = channels if sum_res else 2 * channels\n",
        "\n",
        "        self.up_convs = torch.nn.ModuleList()\n",
        "        for _ in range(depth - 1):\n",
        "            self.up_convs.append(GCNConv(in_channels, channels, improved=True))\n",
        "        self.up_convs.append(GCNConv(in_channels, out_channels, improved=True))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
        "        for conv in self.down_convs:\n",
        "            conv.reset_parameters()\n",
        "        for pool in self.pools:\n",
        "            pool.reset_parameters()\n",
        "        for conv in self.up_convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        edge_index: Tensor,\n",
        "        batch: OptTensor = None,\n",
        "        edge_weight: Tensor = None,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\"\"\"  # noqa: D419\n",
        "        if batch is None:\n",
        "            batch = edge_index.new_zeros(x.size(0))\n",
        "\n",
        "        if edge_weight is None:\n",
        "            edge_weight = x.new_ones(edge_index.size(1))\n",
        "        assert edge_weight.dim() == 1\n",
        "        assert edge_weight.size(0) == edge_index.size(1)\n",
        "\n",
        "        x = self.down_convs[0](x, edge_index, edge_weight)\n",
        "        x = self.act(x)\n",
        "\n",
        "        xs = [x]\n",
        "        edge_indices = [edge_index]\n",
        "        edge_weights = [edge_weight]\n",
        "        perms = []\n",
        "\n",
        "        for i in range(1, self.depth + 1):\n",
        "            edge_index, edge_weight = self.augment_adj(edge_index, edge_weight,\n",
        "                                                       x.size(0))\n",
        "            x, edge_index, edge_weight, batch, perm, _ = self.pools[i - 1](\n",
        "                x, edge_index, edge_weight, batch)\n",
        "\n",
        "            x = self.down_convs[i](x, edge_index, edge_weight)\n",
        "            x = self.act(x)\n",
        "\n",
        "            if i < self.depth:\n",
        "                xs += [x]\n",
        "                edge_indices += [edge_index]\n",
        "                edge_weights += [edge_weight]\n",
        "            perms += [perm]\n",
        "\n",
        "        for i in range(self.depth):\n",
        "            j = self.depth - 1 - i\n",
        "\n",
        "            res = xs[j]\n",
        "            edge_index = edge_indices[j]\n",
        "            edge_weight = edge_weights[j]\n",
        "            perm = perms[j]\n",
        "\n",
        "            up = torch.zeros_like(res)\n",
        "            up[perm] = x\n",
        "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
        "\n",
        "            x = self.up_convs[i](x, edge_index, edge_weight)\n",
        "            x = self.act(x) if i < self.depth - 1 else x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def augment_adj(self, edge_index: Tensor, edge_weight: Tensor,\n",
        "                    num_nodes: int) -> PairTensor:\n",
        "        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n",
        "        edge_index, edge_weight = add_self_loops(edge_index, edge_weight,\n",
        "                                                 num_nodes=num_nodes)\n",
        "        adj = to_torch_csr_tensor(edge_index, edge_weight,\n",
        "                                  size=(num_nodes, num_nodes))\n",
        "        adj = (adj @ adj).to_sparse_coo()\n",
        "        edge_index, edge_weight = adj.indices(), adj.values()\n",
        "        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)\n",
        "        return edge_index, edge_weight\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
        "                f'{self.hidden_channels}, {self.out_channels}, '\n",
        "                f'depth={self.depth}, pool_ratios={self.pool_ratios})')"
      ],
      "metadata": {
        "id": "b8kp2ACgX6TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth 4"
      ],
      "metadata": {
        "id": "AtAaPz55xgMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import dropout_edge\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = GraphUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=4, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9pJPHjsXzuX",
        "outputId": "132e54f2-0532-415f-ae7d-46921316cdaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/utils/sparse.py:276: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  adj = torch.sparse_csr_tensor(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.2000, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 002, Train: 0.2786, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 003, Train: 0.1929, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 004, Train: 0.2214, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 005, Train: 0.2357, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 006, Train: 0.3143, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 007, Train: 0.4357, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 008, Train: 0.4857, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 009, Train: 0.4571, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 010, Train: 0.2357, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 011, Train: 0.2286, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 012, Train: 0.2071, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 013, Train: 0.2500, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 014, Train: 0.2714, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 015, Train: 0.2643, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 016, Train: 0.3143, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 017, Train: 0.4643, Val: 0.3200, Test: 0.3270\n",
            "Epoch: 018, Train: 0.6286, Val: 0.4380, Test: 0.4510\n",
            "Epoch: 019, Train: 0.6429, Val: 0.4500, Test: 0.4570\n",
            "Epoch: 020, Train: 0.5929, Val: 0.4500, Test: 0.4570\n",
            "Epoch: 021, Train: 0.5786, Val: 0.4500, Test: 0.4570\n",
            "Epoch: 022, Train: 0.6000, Val: 0.4500, Test: 0.4570\n",
            "Epoch: 023, Train: 0.6357, Val: 0.4500, Test: 0.4570\n",
            "Epoch: 024, Train: 0.6643, Val: 0.4500, Test: 0.4570\n",
            "Epoch: 025, Train: 0.6786, Val: 0.4500, Test: 0.4570\n",
            "Epoch: 026, Train: 0.7071, Val: 0.4500, Test: 0.4570\n",
            "Epoch: 027, Train: 0.7071, Val: 0.4560, Test: 0.4830\n",
            "Epoch: 028, Train: 0.7286, Val: 0.5060, Test: 0.5320\n",
            "Epoch: 029, Train: 0.8143, Val: 0.5480, Test: 0.5720\n",
            "Epoch: 030, Train: 0.8500, Val: 0.5680, Test: 0.5830\n",
            "Epoch: 031, Train: 0.8857, Val: 0.5680, Test: 0.5830\n",
            "Epoch: 032, Train: 0.8571, Val: 0.5760, Test: 0.5840\n",
            "Epoch: 033, Train: 0.8357, Val: 0.5760, Test: 0.5840\n",
            "Epoch: 034, Train: 0.8286, Val: 0.5760, Test: 0.5840\n",
            "Epoch: 035, Train: 0.8143, Val: 0.5760, Test: 0.5840\n",
            "Epoch: 036, Train: 0.8143, Val: 0.5760, Test: 0.5840\n",
            "Epoch: 037, Train: 0.7857, Val: 0.5760, Test: 0.5840\n",
            "Epoch: 038, Train: 0.8143, Val: 0.5760, Test: 0.5840\n",
            "Epoch: 039, Train: 0.8571, Val: 0.5760, Test: 0.5840\n",
            "Epoch: 040, Train: 0.8786, Val: 0.5760, Test: 0.5840\n",
            "Epoch: 041, Train: 0.8786, Val: 0.5860, Test: 0.5890\n",
            "Epoch: 042, Train: 0.8571, Val: 0.5860, Test: 0.5890\n",
            "Epoch: 043, Train: 0.8571, Val: 0.6000, Test: 0.6080\n",
            "Epoch: 044, Train: 0.8571, Val: 0.6360, Test: 0.6300\n",
            "Epoch: 045, Train: 0.8143, Val: 0.6360, Test: 0.6300\n",
            "Epoch: 046, Train: 0.8500, Val: 0.6540, Test: 0.6470\n",
            "Epoch: 047, Train: 0.8786, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 048, Train: 0.8643, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 049, Train: 0.8714, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 050, Train: 0.8643, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 051, Train: 0.8714, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 052, Train: 0.8714, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 053, Train: 0.8429, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 054, Train: 0.8571, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 055, Train: 0.8286, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 056, Train: 0.8000, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 057, Train: 0.8500, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 058, Train: 0.8714, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 059, Train: 0.8500, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 060, Train: 0.8214, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 061, Train: 0.8143, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 062, Train: 0.8143, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 063, Train: 0.8429, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 064, Train: 0.8500, Val: 0.6820, Test: 0.6740\n",
            "Epoch: 065, Train: 0.8643, Val: 0.7200, Test: 0.7050\n",
            "Epoch: 066, Train: 0.8714, Val: 0.7200, Test: 0.7050\n",
            "Epoch: 067, Train: 0.8500, Val: 0.7200, Test: 0.7050\n",
            "Epoch: 068, Train: 0.8429, Val: 0.7200, Test: 0.7050\n",
            "Epoch: 069, Train: 0.8929, Val: 0.7480, Test: 0.7220\n",
            "Epoch: 070, Train: 0.8643, Val: 0.7480, Test: 0.7220\n",
            "Epoch: 071, Train: 0.8857, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 072, Train: 0.8929, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 073, Train: 0.8786, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 074, Train: 0.8929, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 075, Train: 0.9143, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 076, Train: 0.9143, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 077, Train: 0.9071, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 078, Train: 0.9071, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 079, Train: 0.9071, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 080, Train: 0.9071, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 081, Train: 0.9214, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 082, Train: 0.9214, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 083, Train: 0.9286, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 084, Train: 0.9214, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 085, Train: 0.9429, Val: 0.7540, Test: 0.7300\n",
            "Epoch: 086, Train: 0.9500, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 087, Train: 0.9571, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 088, Train: 0.9643, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 089, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 090, Train: 0.9500, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 091, Train: 0.9500, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 092, Train: 0.9571, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 093, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 094, Train: 0.9214, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 095, Train: 0.9214, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 096, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 097, Train: 0.9143, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 098, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 099, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 100, Train: 0.9286, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 101, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 102, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 103, Train: 0.9214, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 104, Train: 0.9429, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 105, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 106, Train: 0.9429, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 107, Train: 0.8929, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 108, Train: 0.9286, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 109, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 110, Train: 0.8929, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 111, Train: 0.8857, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 112, Train: 0.8929, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 113, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 114, Train: 0.9286, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 115, Train: 0.9357, Val: 0.7780, Test: 0.7710\n",
            "Epoch: 116, Train: 0.9286, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 117, Train: 0.9357, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 118, Train: 0.9357, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 119, Train: 0.9357, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 120, Train: 0.9286, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 121, Train: 0.9429, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 122, Train: 0.9429, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 123, Train: 0.9571, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 124, Train: 0.9571, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 125, Train: 0.9571, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 126, Train: 0.9643, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 127, Train: 0.9571, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 128, Train: 0.9357, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 129, Train: 0.9500, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 130, Train: 0.9571, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 131, Train: 0.9571, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 132, Train: 0.9571, Val: 0.7840, Test: 0.7820\n",
            "Epoch: 133, Train: 0.9714, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 134, Train: 0.9429, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 135, Train: 0.9357, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 136, Train: 0.9143, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 137, Train: 0.9357, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 138, Train: 0.9286, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 139, Train: 0.9143, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 140, Train: 0.9000, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 141, Train: 0.8786, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 142, Train: 0.8500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 143, Train: 0.8929, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 144, Train: 0.9143, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 145, Train: 0.9000, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 146, Train: 0.9071, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 147, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 148, Train: 0.9357, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 149, Train: 0.9429, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 150, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 151, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 152, Train: 0.9143, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 153, Train: 0.9357, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 154, Train: 0.9357, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 155, Train: 0.9143, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 156, Train: 0.9143, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 157, Train: 0.9429, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 158, Train: 0.9571, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 159, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 160, Train: 0.9643, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 161, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 162, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 163, Train: 0.9214, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 164, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 165, Train: 0.9643, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 166, Train: 0.9571, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 167, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 168, Train: 0.9571, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 169, Train: 0.9429, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 170, Train: 0.9357, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 171, Train: 0.9357, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 172, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 173, Train: 0.9429, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 174, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 175, Train: 0.9429, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 176, Train: 0.9429, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 177, Train: 0.9571, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 178, Train: 0.9500, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 179, Train: 0.9286, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 180, Train: 0.9643, Val: 0.7860, Test: 0.8020\n",
            "Epoch: 181, Train: 0.9571, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 182, Train: 0.9214, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 183, Train: 0.9071, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 184, Train: 0.9214, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 185, Train: 0.9286, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 186, Train: 0.9429, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 187, Train: 0.9571, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 188, Train: 0.9500, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 189, Train: 0.9643, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 190, Train: 0.9643, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 191, Train: 0.9643, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 192, Train: 0.9500, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 193, Train: 0.9643, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 194, Train: 0.9500, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 195, Train: 0.9500, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 196, Train: 0.9429, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 197, Train: 0.9214, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 198, Train: 0.9286, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 199, Train: 0.9071, Val: 0.7940, Test: 0.8010\n",
            "Epoch: 200, Train: 0.9500, Val: 0.7940, Test: 0.8010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth 5"
      ],
      "metadata": {
        "id": "cv7lrMW7xQ1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import dropout_edge\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = GraphUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=5, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZIdJX_AxKBS",
        "outputId": "a3a5ed52-45ce-466b-8455-d39be8dc6e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.1786, Val: 0.1240, Test: 0.1280\n",
            "Epoch: 002, Train: 0.1571, Val: 0.1240, Test: 0.1280\n",
            "Epoch: 003, Train: 0.1643, Val: 0.1240, Test: 0.1280\n",
            "Epoch: 004, Train: 0.2786, Val: 0.1500, Test: 0.1770\n",
            "Epoch: 005, Train: 0.3786, Val: 0.2080, Test: 0.2260\n",
            "Epoch: 006, Train: 0.4357, Val: 0.2320, Test: 0.2530\n",
            "Epoch: 007, Train: 0.4643, Val: 0.2320, Test: 0.2530\n",
            "Epoch: 008, Train: 0.5143, Val: 0.2680, Test: 0.2940\n",
            "Epoch: 009, Train: 0.5143, Val: 0.2980, Test: 0.3170\n",
            "Epoch: 010, Train: 0.5571, Val: 0.3440, Test: 0.3490\n",
            "Epoch: 011, Train: 0.6071, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 012, Train: 0.5929, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 013, Train: 0.4143, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 014, Train: 0.4429, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 015, Train: 0.5571, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 016, Train: 0.4929, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 017, Train: 0.5286, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 018, Train: 0.5000, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 019, Train: 0.5643, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 020, Train: 0.6286, Val: 0.3700, Test: 0.3810\n",
            "Epoch: 021, Train: 0.7071, Val: 0.4040, Test: 0.4150\n",
            "Epoch: 022, Train: 0.7214, Val: 0.4420, Test: 0.4440\n",
            "Epoch: 023, Train: 0.7286, Val: 0.4420, Test: 0.4440\n",
            "Epoch: 024, Train: 0.7286, Val: 0.4420, Test: 0.4440\n",
            "Epoch: 025, Train: 0.7429, Val: 0.4480, Test: 0.4380\n",
            "Epoch: 026, Train: 0.7643, Val: 0.4480, Test: 0.4380\n",
            "Epoch: 027, Train: 0.7214, Val: 0.4480, Test: 0.4380\n",
            "Epoch: 028, Train: 0.6786, Val: 0.4480, Test: 0.4380\n",
            "Epoch: 029, Train: 0.6714, Val: 0.4480, Test: 0.4380\n",
            "Epoch: 030, Train: 0.7071, Val: 0.4480, Test: 0.4380\n",
            "Epoch: 031, Train: 0.7286, Val: 0.4480, Test: 0.4380\n",
            "Epoch: 032, Train: 0.7143, Val: 0.4620, Test: 0.4740\n",
            "Epoch: 033, Train: 0.7357, Val: 0.4860, Test: 0.4800\n",
            "Epoch: 034, Train: 0.6571, Val: 0.4860, Test: 0.4800\n",
            "Epoch: 035, Train: 0.6500, Val: 0.4860, Test: 0.4800\n",
            "Epoch: 036, Train: 0.7286, Val: 0.4880, Test: 0.5140\n",
            "Epoch: 037, Train: 0.7643, Val: 0.5200, Test: 0.5330\n",
            "Epoch: 038, Train: 0.7786, Val: 0.5240, Test: 0.5490\n",
            "Epoch: 039, Train: 0.7643, Val: 0.5720, Test: 0.5770\n",
            "Epoch: 040, Train: 0.7571, Val: 0.5720, Test: 0.5770\n",
            "Epoch: 041, Train: 0.8000, Val: 0.5800, Test: 0.5880\n",
            "Epoch: 042, Train: 0.7786, Val: 0.5800, Test: 0.5880\n",
            "Epoch: 043, Train: 0.7500, Val: 0.5800, Test: 0.5880\n",
            "Epoch: 044, Train: 0.7714, Val: 0.5800, Test: 0.5880\n",
            "Epoch: 045, Train: 0.7714, Val: 0.5800, Test: 0.5880\n",
            "Epoch: 046, Train: 0.7929, Val: 0.5800, Test: 0.5880\n",
            "Epoch: 047, Train: 0.8000, Val: 0.5800, Test: 0.5880\n",
            "Epoch: 048, Train: 0.8357, Val: 0.5800, Test: 0.5880\n",
            "Epoch: 049, Train: 0.8500, Val: 0.6020, Test: 0.6100\n",
            "Epoch: 050, Train: 0.8429, Val: 0.6020, Test: 0.6100\n",
            "Epoch: 051, Train: 0.8643, Val: 0.6020, Test: 0.6100\n",
            "Epoch: 052, Train: 0.8571, Val: 0.6100, Test: 0.6620\n",
            "Epoch: 053, Train: 0.8571, Val: 0.6180, Test: 0.6700\n",
            "Epoch: 054, Train: 0.8714, Val: 0.6440, Test: 0.6820\n",
            "Epoch: 055, Train: 0.8786, Val: 0.6440, Test: 0.6820\n",
            "Epoch: 056, Train: 0.8929, Val: 0.6620, Test: 0.6900\n",
            "Epoch: 057, Train: 0.8857, Val: 0.6660, Test: 0.7030\n",
            "Epoch: 058, Train: 0.9143, Val: 0.6660, Test: 0.7030\n",
            "Epoch: 059, Train: 0.9143, Val: 0.6660, Test: 0.7030\n",
            "Epoch: 060, Train: 0.8714, Val: 0.6660, Test: 0.7030\n",
            "Epoch: 061, Train: 0.8500, Val: 0.6660, Test: 0.7030\n",
            "Epoch: 062, Train: 0.8929, Val: 0.6660, Test: 0.7030\n",
            "Epoch: 063, Train: 0.8786, Val: 0.6660, Test: 0.7030\n",
            "Epoch: 064, Train: 0.9071, Val: 0.6660, Test: 0.7030\n",
            "Epoch: 065, Train: 0.9143, Val: 0.6660, Test: 0.7030\n",
            "Epoch: 066, Train: 0.9000, Val: 0.6760, Test: 0.6940\n",
            "Epoch: 067, Train: 0.8929, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 068, Train: 0.8929, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 069, Train: 0.9071, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 070, Train: 0.8643, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 071, Train: 0.8929, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 072, Train: 0.8714, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 073, Train: 0.8286, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 074, Train: 0.8643, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 075, Train: 0.8571, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 076, Train: 0.8714, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 077, Train: 0.8571, Val: 0.7040, Test: 0.7190\n",
            "Epoch: 078, Train: 0.8786, Val: 0.7400, Test: 0.7730\n",
            "Epoch: 079, Train: 0.8929, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 080, Train: 0.8857, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 081, Train: 0.8857, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 082, Train: 0.9000, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 083, Train: 0.9143, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 084, Train: 0.9357, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 085, Train: 0.9500, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 086, Train: 0.9500, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 087, Train: 0.9429, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 088, Train: 0.9429, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 089, Train: 0.9214, Val: 0.7560, Test: 0.7720\n",
            "Epoch: 090, Train: 0.9429, Val: 0.7660, Test: 0.7810\n",
            "Epoch: 091, Train: 0.9500, Val: 0.7660, Test: 0.7810\n",
            "Epoch: 092, Train: 0.9643, Val: 0.7660, Test: 0.7810\n",
            "Epoch: 093, Train: 0.9500, Val: 0.7660, Test: 0.7810\n",
            "Epoch: 094, Train: 0.9429, Val: 0.7660, Test: 0.7810\n",
            "Epoch: 095, Train: 0.9429, Val: 0.7660, Test: 0.7810\n",
            "Epoch: 096, Train: 0.9429, Val: 0.7660, Test: 0.7810\n",
            "Epoch: 097, Train: 0.9714, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 098, Train: 0.9643, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 099, Train: 0.9571, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 100, Train: 0.9571, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 101, Train: 0.9500, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 102, Train: 0.9643, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 103, Train: 0.9643, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 104, Train: 0.9714, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 105, Train: 0.9714, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 106, Train: 0.9643, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 107, Train: 0.9571, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 108, Train: 0.9357, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 109, Train: 0.9357, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 110, Train: 0.9357, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 111, Train: 0.9643, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 112, Train: 0.9714, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 113, Train: 0.9643, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 114, Train: 0.9571, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 115, Train: 0.9714, Val: 0.7680, Test: 0.7670\n",
            "Epoch: 116, Train: 0.9714, Val: 0.7700, Test: 0.7880\n",
            "Epoch: 117, Train: 0.9571, Val: 0.7720, Test: 0.7640\n",
            "Epoch: 118, Train: 0.9571, Val: 0.7720, Test: 0.7640\n",
            "Epoch: 119, Train: 0.9643, Val: 0.7720, Test: 0.7640\n",
            "Epoch: 120, Train: 0.9714, Val: 0.7720, Test: 0.7640\n",
            "Epoch: 121, Train: 0.9714, Val: 0.7800, Test: 0.7920\n",
            "Epoch: 122, Train: 0.9643, Val: 0.7860, Test: 0.7970\n",
            "Epoch: 123, Train: 0.9429, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 124, Train: 0.9643, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 125, Train: 0.9929, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 126, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 127, Train: 0.9643, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 128, Train: 0.9500, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 129, Train: 0.9571, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 130, Train: 0.9643, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 131, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 132, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 133, Train: 0.9643, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 134, Train: 0.9429, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 135, Train: 0.9500, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 136, Train: 0.9357, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 137, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 138, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 139, Train: 0.9643, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 140, Train: 0.9500, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 141, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 142, Train: 0.9500, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 143, Train: 0.9571, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 144, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 145, Train: 0.9571, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 146, Train: 0.9429, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 147, Train: 0.9643, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 148, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 149, Train: 0.9857, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 150, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 151, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 152, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 153, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 154, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 155, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 156, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 157, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 158, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 159, Train: 0.9929, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 160, Train: 0.9929, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 161, Train: 0.9857, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 162, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 163, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 164, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 165, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 166, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 167, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 168, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 169, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 170, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 171, Train: 0.9714, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 172, Train: 0.9786, Val: 0.7900, Test: 0.8050\n",
            "Epoch: 173, Train: 0.9714, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 174, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 175, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 176, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 177, Train: 0.9643, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 178, Train: 0.9643, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 179, Train: 0.9643, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 180, Train: 0.9571, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 181, Train: 0.9643, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 182, Train: 0.9571, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 183, Train: 0.9500, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 184, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 185, Train: 0.9857, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 186, Train: 0.9643, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 187, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 188, Train: 0.9571, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 189, Train: 0.9571, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 190, Train: 0.9571, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 191, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 192, Train: 0.9857, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 193, Train: 0.9857, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 194, Train: 0.9857, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 195, Train: 0.9857, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 196, Train: 0.9857, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 197, Train: 0.9786, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 198, Train: 0.9714, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 199, Train: 0.9714, Val: 0.7940, Test: 0.8090\n",
            "Epoch: 200, Train: 0.9714, Val: 0.7940, Test: 0.8090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth 13"
      ],
      "metadata": {
        "id": "Abw6qaKKxsPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import dropout_edge\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = GraphUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=13, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "q1e1Ljqkxtlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth 50"
      ],
      "metadata": {
        "id": "2q2dujzyxwb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import dropout_edge\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = GraphUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=50, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kr4Bk05xyY_",
        "outputId": "9ed6ec07-7334-422b-abda-f74171e14bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 002, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 003, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 004, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 005, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 006, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 007, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 008, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 009, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 010, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 011, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 012, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 013, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 014, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 015, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 016, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 017, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 018, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 019, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 020, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 021, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 022, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 023, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 024, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 025, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 026, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 027, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 028, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 029, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 030, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 031, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 032, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 033, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 034, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 035, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 036, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 037, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 038, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 039, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 040, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 041, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 042, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 043, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 044, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 045, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 046, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 047, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 048, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 049, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 050, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 051, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 052, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 053, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 054, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 055, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 056, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 057, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 058, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 059, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 060, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 061, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 062, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 063, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 064, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 065, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 066, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 067, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 068, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 069, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 070, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 071, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 072, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 073, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 074, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 075, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 076, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 077, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 078, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 079, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 080, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 081, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 082, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 083, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 084, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 085, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 086, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 087, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 088, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 089, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 090, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 091, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 092, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 093, Train: 0.1429, Val: 0.1220, Test: 0.1300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depth 100"
      ],
      "metadata": {
        "id": "mO_GdMORx1K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import dropout_edge\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = GraphUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=100, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbZiN8DZx3Hg",
        "outputId": "2466abd2-177c-429e-f53f-c267d9cb56c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/utils/sparse.py:276: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  adj = torch.sparse_csr_tensor(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 002, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 003, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 004, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 005, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 006, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 007, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 008, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 009, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 010, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 011, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 012, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 013, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 014, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 015, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 016, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 017, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 018, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 019, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 020, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 021, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 022, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 023, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 024, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 025, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 026, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 027, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 028, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 029, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 030, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 031, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 032, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 033, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 034, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 035, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 036, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 037, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 038, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 039, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 040, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 041, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 042, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 043, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 044, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 045, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 046, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 047, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 048, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 049, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 050, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 051, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 052, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 053, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 054, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 055, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 056, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 057, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 058, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 059, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 060, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 061, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 062, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 063, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 064, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 065, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 066, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 067, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 068, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 069, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 070, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 071, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 072, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 073, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 074, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 075, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 076, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 077, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 078, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 079, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 080, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 081, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 082, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 083, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 084, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 085, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 086, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 087, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 088, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 089, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 090, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 091, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 092, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 093, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 094, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 095, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 096, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 097, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 098, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 099, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 100, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 101, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 102, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 103, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 104, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 105, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 106, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 107, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 108, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 109, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 110, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 111, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 112, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 113, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 114, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 115, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 116, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 117, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 118, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 119, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 120, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 121, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 122, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 123, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 124, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 125, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 126, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 127, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 128, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 129, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 130, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 131, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 132, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 133, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 134, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 135, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 136, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 137, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 138, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 139, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 140, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 141, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 142, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 143, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 144, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 145, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 146, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 147, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 148, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 149, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 150, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 151, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 152, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 153, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 154, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 155, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 156, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 157, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 158, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 159, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 160, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 161, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 162, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 163, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 164, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 165, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 166, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 167, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 168, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 169, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 170, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 171, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 172, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 173, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 174, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 175, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 176, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 177, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 178, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 179, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 180, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 181, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 182, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 183, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 184, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 185, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 186, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 187, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 188, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 189, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 190, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 191, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 192, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 193, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 194, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 195, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 196, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 197, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 198, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 199, Train: 0.1429, Val: 0.1220, Test: 0.1300\n",
            "Epoch: 200, Train: 0.1429, Val: 0.1220, Test: 0.1300\n"
          ]
        }
      ]
    }
  ]
}