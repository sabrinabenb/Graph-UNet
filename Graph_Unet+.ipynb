{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabrinabenb/Graph-UNet/blob/main/Graph_Unet%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this work we combine several innovative elements to\n",
        "boost its effectiveness. It utilizes data enrichment methods to compensate\n",
        "for incomplete data, refined optimization approaches to improve learning efficiency,\n",
        "and targeted regularization strategies to address common GNN limitations.\n",
        "These elements work in concert to overcome fundamental challenges\n",
        "in graph representation learning while maintaining scalability across various\n",
        "graph sizes and types"
      ],
      "metadata": {
        "id": "yAvcUDRRzIdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "os.environ['PYTHONWARNINGS'] = \"ignore\"\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "TWSi-qy_Bf2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, List, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch_sparse import spspmm\n",
        "\n",
        "from torch_geometric.typing import OptTensor, PairTensor\n",
        "from torch_geometric.utils import (\n",
        "    add_self_loops,\n",
        "    remove_self_loops,\n",
        "    sort_edge_index,\n",
        ")\n",
        "from torch_geometric.utils.repeat import repeat\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "Z6tljGVtHEAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "dataset = Planetoid(root='data/Planetoid', name='citeseer', transform=NormalizeFeatures())\n",
        "\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]\n",
        "g = dataset[0] # Get the first graph object.\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR4fl8gjHhOw",
        "outputId": "d3d2c557-3010-4ac4-cdd8-58cee7aa49c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: citeseer():\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 3703\n",
            "Number of classes: 6\n",
            "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#New version of Graph UNets\n",
        "\n",
        "DeepGUNet model combining GCNConv, SAGPooling, and LayerNorm"
      ],
      "metadata": {
        "id": "qnf9IlHbHzoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, List, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch_sparse import spspmm\n",
        "\n",
        "from torch_geometric.nn import GCNConv, SAGPooling,TopKPooling,GATConv,SSGConv, LayerNorm # Import LayerNorm\n",
        "from torch_geometric.typing import OptTensor, PairTensor\n",
        "from torch_geometric.utils import (\n",
        "    add_self_loops,\n",
        "    remove_self_loops,\n",
        "    sort_edge_index,\n",
        ")\n",
        "from torch_geometric.utils.repeat import repeat\n",
        "\n",
        "class DeepGUNet(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        hidden_channels: int,\n",
        "        out_channels: int,\n",
        "        depth= int,\n",
        "        pool_ratios: Union[float, List[float]] = 0.5,\n",
        "        sum_res: bool = True,\n",
        "        act: Callable = F.relu,\n",
        "        dropout: float = 0.5 # Added dropout rate parameter\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.depth = depth\n",
        "        self.pool_ratios = repeat(pool_ratios, depth)\n",
        "        self.act = act\n",
        "        self.sum_res = sum_res\n",
        "        self.dropout = dropout # Store dropout rate\n",
        "\n",
        "        channels = hidden_channels\n",
        "\n",
        "        self.down_convs = torch.nn.ModuleList()\n",
        "        self.pools = torch.nn.ModuleList()\n",
        "        self.down_convs.append(GCNConv(in_channels, channels,improved=True))\n",
        "        self.norms = torch.nn.ModuleList()\n",
        "        self.norms.append(LayerNorm(channels))\n",
        "        # Added dropout layers for downsampling\n",
        "        self.down_dropouts = torch.nn.ModuleList()\n",
        "        self.down_dropouts.append(torch.nn.Dropout(self.dropout))\n",
        "\n",
        "        for i in range(depth):\n",
        "            self.pools.append(SAGPooling(channels, self.pool_ratios[i]))\n",
        "            self.down_convs.append(GCNConv(channels, channels,improved=True))\n",
        "            self.norms.append(LayerNorm(channels))\n",
        "            self.down_dropouts.append(torch.nn.Dropout(self.dropout)) # Added dropout layers\n",
        "\n",
        "\n",
        "        in_channels = channels if sum_res else 2 * channels\n",
        "\n",
        "        self.up_convs = torch.nn.ModuleList()\n",
        "        # Added dropout layers for upsampling\n",
        "        self.up_dropouts = torch.nn.ModuleList()\n",
        "        for i in range(depth): # Need dropout for each up_conv including the last one\n",
        "             self.up_convs.append(GCNConv(in_channels, channels,improved=True) if i < depth -1 else SSGConv(in_channels, out_channels,alpha=0.3))\n",
        "             self.up_dropouts.append(torch.nn.Dropout(self.dropout))\n",
        "\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.down_convs:\n",
        "            conv.reset_parameters()\n",
        "        for pool in self.pools:\n",
        "            pool.reset_parameters()\n",
        "        for conv in self.up_convs:\n",
        "            conv.reset_parameters()\n",
        "        for norm in self.norms:\n",
        "            norm.reset_parameters()\n",
        "        # Removed reset_parameters() calls for dropout layers\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Tensor,\n",
        "                batch: OptTensor = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if batch is None:\n",
        "            batch = edge_index.new_zeros(x.size(0))\n",
        "        edge_weight = x.new_ones(edge_index.size(1))\n",
        "\n",
        "        x = self.down_convs[0](x, edge_index, edge_weight)\n",
        "        x = self.norms[0](x)\n",
        "        x = self.act(x)\n",
        "        x = self.down_dropouts[0](x) # Applied dropout\n",
        "\n",
        "        xs = [x]\n",
        "        edge_indices = [edge_index]\n",
        "        edge_weights = [edge_weight]\n",
        "        perms = []\n",
        "\n",
        "        for i in range(1, self.depth + 1):\n",
        "\n",
        "            x, edge_index, edge_weight, batch, perm, _ = self.pools[i - 1](\n",
        "                x, edge_index, edge_weight, batch)\n",
        "\n",
        "            x = self.down_convs[i](x, edge_index, edge_weight)\n",
        "            x = self.norms[i](x)\n",
        "            x = self.act(x)\n",
        "            x = self.down_dropouts[i](x) # Applied dropout\n",
        "\n",
        "\n",
        "            if i < self.depth:\n",
        "                xs += [x]\n",
        "                edge_indices += [edge_index]\n",
        "                edge_weights += [edge_weight]\n",
        "            perms += [perm]\n",
        "\n",
        "        for i in range(self.depth):\n",
        "            j = self.depth - 1 - i\n",
        "\n",
        "            res = xs[j]\n",
        "            edge_index = edge_indices[j]\n",
        "            edge_weight = edge_weights[j]\n",
        "            perm = perms[j]\n",
        "\n",
        "            up = torch.zeros_like(res)\n",
        "            up[perm] = x\n",
        "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
        "\n",
        "            x = self.up_convs[i](x, edge_index, edge_weight)\n",
        "\n",
        "            # Applied dropout after activation in upsampling\n",
        "            x = self.act(x) if i < self.depth - 1 else x\n",
        "            x = self.up_dropouts[i](x) if i < self.depth -1 else x # Only apply dropout before the final output layer\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_T0zxb5dHy7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.utils import dropout_edge"
      ],
      "metadata": {
        "id": "U9eL62sKIDmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#depth=3"
      ],
      "metadata": {
        "id": "l9SAzrdcySRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = DeepGUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=3, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7Yqdq65IJug",
        "outputId": "41c35823-b4e0-472f-d837-d73d271ff3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.2083, Val: 0.1960, Test: 0.2230\n",
            "Epoch: 002, Train: 0.3167, Val: 0.1960, Test: 0.2230\n",
            "Epoch: 003, Train: 0.3583, Val: 0.2480, Test: 0.2340\n",
            "Epoch: 004, Train: 0.4417, Val: 0.2820, Test: 0.2980\n",
            "Epoch: 005, Train: 0.5500, Val: 0.3340, Test: 0.3470\n",
            "Epoch: 006, Train: 0.6667, Val: 0.3860, Test: 0.3970\n",
            "Epoch: 007, Train: 0.7083, Val: 0.4000, Test: 0.4050\n",
            "Epoch: 008, Train: 0.6750, Val: 0.4040, Test: 0.3930\n",
            "Epoch: 009, Train: 0.7167, Val: 0.4100, Test: 0.4260\n",
            "Epoch: 010, Train: 0.7667, Val: 0.4580, Test: 0.4700\n",
            "Epoch: 011, Train: 0.8167, Val: 0.4780, Test: 0.4830\n",
            "Epoch: 012, Train: 0.8083, Val: 0.5120, Test: 0.5220\n",
            "Epoch: 013, Train: 0.8750, Val: 0.5760, Test: 0.5600\n",
            "Epoch: 014, Train: 0.9083, Val: 0.6220, Test: 0.6180\n",
            "Epoch: 015, Train: 0.9083, Val: 0.6480, Test: 0.6600\n",
            "Epoch: 016, Train: 0.8917, Val: 0.6680, Test: 0.6770\n",
            "Epoch: 017, Train: 0.9083, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 018, Train: 0.9167, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 019, Train: 0.9333, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 020, Train: 0.9583, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 021, Train: 0.9583, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 022, Train: 0.9583, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 023, Train: 0.9500, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 024, Train: 0.9667, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 025, Train: 0.9667, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 026, Train: 0.9667, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 027, Train: 0.9750, Val: 0.6740, Test: 0.6890\n",
            "Epoch: 028, Train: 0.9667, Val: 0.6840, Test: 0.6840\n",
            "Epoch: 029, Train: 0.9667, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 030, Train: 0.9583, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 031, Train: 0.9417, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 032, Train: 0.9417, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 033, Train: 0.9500, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 034, Train: 0.9500, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 035, Train: 0.9667, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 036, Train: 0.9750, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 037, Train: 0.9667, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 038, Train: 0.9750, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 039, Train: 0.9750, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 040, Train: 0.9833, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 041, Train: 0.9833, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 042, Train: 0.9750, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 043, Train: 0.9500, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 044, Train: 0.9667, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 045, Train: 0.9667, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 046, Train: 0.9667, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 047, Train: 0.9667, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 048, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 049, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 050, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 051, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 052, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 053, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 054, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 055, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 056, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 057, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 058, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 059, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 060, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 061, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 062, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 063, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 064, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 065, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 066, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 067, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 068, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 069, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 070, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 071, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 072, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 073, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 074, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 075, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 076, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 077, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 078, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 079, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 080, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 081, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 082, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 083, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 084, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 085, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 086, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 087, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 088, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 089, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 090, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 091, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 092, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 093, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 094, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 095, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 096, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 097, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 098, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 099, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 100, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 101, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 102, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 103, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 104, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 105, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 106, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 107, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 108, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 109, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 110, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 111, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 112, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 113, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 114, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 115, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 116, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 117, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 118, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 119, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 120, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 121, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 122, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 123, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 124, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 125, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 126, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 127, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 128, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 129, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 130, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 131, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 132, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 133, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 134, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 135, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 136, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 137, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 138, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 139, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 140, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 141, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 142, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 143, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 144, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 145, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 146, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 147, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 148, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 149, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 150, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 151, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 152, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 153, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 154, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 155, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 156, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 157, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 158, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 159, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 160, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 161, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 162, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 163, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 164, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 165, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 166, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 167, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 168, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 169, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 170, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 171, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 172, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 173, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 174, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 175, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 176, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 177, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 178, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 179, Train: 0.9917, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 180, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 181, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 182, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 183, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 184, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 185, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 186, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 187, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 188, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 189, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 190, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 191, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 192, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 193, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 194, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 195, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 196, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 197, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 198, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 199, Train: 1.0000, Val: 0.7040, Test: 0.6870\n",
            "Epoch: 200, Train: 1.0000, Val: 0.7040, Test: 0.6870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth 5"
      ],
      "metadata": {
        "id": "TbwD3Jt1yV_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = DeepGUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=5, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lANI9NUyIRJx",
        "outputId": "c39ed46c-c3ec-4396-e815-bccb059c4618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.2500, Val: 0.1580, Test: 0.2210\n",
            "Epoch: 002, Train: 0.2250, Val: 0.1580, Test: 0.2210\n",
            "Epoch: 003, Train: 0.3667, Val: 0.1800, Test: 0.2070\n",
            "Epoch: 004, Train: 0.5000, Val: 0.3440, Test: 0.3370\n",
            "Epoch: 005, Train: 0.5417, Val: 0.4020, Test: 0.3660\n",
            "Epoch: 006, Train: 0.5000, Val: 0.4020, Test: 0.3660\n",
            "Epoch: 007, Train: 0.5333, Val: 0.4520, Test: 0.4340\n",
            "Epoch: 008, Train: 0.5833, Val: 0.4600, Test: 0.4730\n",
            "Epoch: 009, Train: 0.6083, Val: 0.4600, Test: 0.4730\n",
            "Epoch: 010, Train: 0.6583, Val: 0.4600, Test: 0.4730\n",
            "Epoch: 011, Train: 0.8083, Val: 0.5480, Test: 0.5650\n",
            "Epoch: 012, Train: 0.8500, Val: 0.5620, Test: 0.5880\n",
            "Epoch: 013, Train: 0.8667, Val: 0.5620, Test: 0.5880\n",
            "Epoch: 014, Train: 0.8667, Val: 0.5620, Test: 0.5880\n",
            "Epoch: 015, Train: 0.8917, Val: 0.5660, Test: 0.5980\n",
            "Epoch: 016, Train: 0.9083, Val: 0.5900, Test: 0.6180\n",
            "Epoch: 017, Train: 0.9333, Val: 0.6220, Test: 0.6410\n",
            "Epoch: 018, Train: 0.9417, Val: 0.6440, Test: 0.6600\n",
            "Epoch: 019, Train: 0.9500, Val: 0.6560, Test: 0.6680\n",
            "Epoch: 020, Train: 0.9500, Val: 0.6620, Test: 0.6620\n",
            "Epoch: 021, Train: 0.9417, Val: 0.6620, Test: 0.6620\n",
            "Epoch: 022, Train: 0.9417, Val: 0.6620, Test: 0.6620\n",
            "Epoch: 023, Train: 0.9417, Val: 0.6620, Test: 0.6620\n",
            "Epoch: 024, Train: 0.9583, Val: 0.6620, Test: 0.6620\n",
            "Epoch: 025, Train: 0.9500, Val: 0.6620, Test: 0.6620\n",
            "Epoch: 026, Train: 0.9583, Val: 0.6620, Test: 0.6620\n",
            "Epoch: 027, Train: 0.9500, Val: 0.6620, Test: 0.6620\n",
            "Epoch: 028, Train: 0.9500, Val: 0.6740, Test: 0.7010\n",
            "Epoch: 029, Train: 0.9500, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 030, Train: 0.9500, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 031, Train: 0.9500, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 032, Train: 0.9583, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 033, Train: 0.9667, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 034, Train: 0.9667, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 035, Train: 0.9667, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 036, Train: 0.9750, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 037, Train: 0.9750, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 038, Train: 0.9750, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 039, Train: 0.9750, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 040, Train: 0.9750, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 041, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 042, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 043, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 044, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 045, Train: 0.9917, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 046, Train: 0.9917, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 047, Train: 0.9917, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 048, Train: 0.9917, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 049, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 050, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 051, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 052, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 053, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 054, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 055, Train: 0.9667, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 056, Train: 0.9833, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 057, Train: 0.9917, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 058, Train: 0.9917, Val: 0.6840, Test: 0.7070\n",
            "Epoch: 059, Train: 0.9917, Val: 0.6860, Test: 0.6480\n",
            "Epoch: 060, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 061, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 062, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 063, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 064, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 065, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 066, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 067, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 068, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 069, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 070, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 071, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 072, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 073, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 074, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 075, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 076, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 077, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 078, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 079, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 080, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 081, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 082, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 083, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 084, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 085, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 086, Train: 0.9750, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 087, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 088, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 089, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 090, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 091, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 092, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 093, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 094, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 095, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 096, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 097, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 098, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 099, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 100, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 101, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 102, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 103, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 104, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 105, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 106, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 107, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 108, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 109, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 110, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 111, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 112, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 113, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 114, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 115, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 116, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 117, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 118, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 119, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 120, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 121, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 122, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 123, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 124, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 125, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 126, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 127, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 128, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 129, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 130, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 131, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 132, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 133, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 134, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 135, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 136, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 137, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 138, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 139, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 140, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 141, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 142, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 143, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 144, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 145, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 146, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 147, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 148, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 149, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 150, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 151, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 152, Train: 0.9833, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 153, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 154, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 155, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 156, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 157, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 158, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 159, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 160, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 161, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 162, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 163, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 164, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 165, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 166, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 167, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 168, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 169, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 170, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 171, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 172, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 173, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 174, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 175, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 176, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 177, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 178, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 179, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 180, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 181, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 182, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 183, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 184, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 185, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 186, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 187, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 188, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 189, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 190, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 191, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 192, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 193, Train: 0.9917, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 194, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 195, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 196, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 197, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 198, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 199, Train: 1.0000, Val: 0.6880, Test: 0.6610\n",
            "Epoch: 200, Train: 1.0000, Val: 0.6880, Test: 0.6610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = DeepGUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=4, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvhg8Ia9dIxn",
        "outputId": "ca015e55-f32f-4c46-d1ce-0c59743e0e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.1750, Val: 0.0540, Test: 0.0780\n",
            "Epoch: 002, Train: 0.2500, Val: 0.0780, Test: 0.0920\n",
            "Epoch: 003, Train: 0.2083, Val: 0.1100, Test: 0.1210\n",
            "Epoch: 004, Train: 0.3750, Val: 0.3220, Test: 0.3230\n",
            "Epoch: 005, Train: 0.4500, Val: 0.3460, Test: 0.3710\n",
            "Epoch: 006, Train: 0.6083, Val: 0.3900, Test: 0.4450\n",
            "Epoch: 007, Train: 0.7167, Val: 0.4460, Test: 0.4700\n",
            "Epoch: 008, Train: 0.7333, Val: 0.4460, Test: 0.4700\n",
            "Epoch: 009, Train: 0.7333, Val: 0.4460, Test: 0.4700\n",
            "Epoch: 010, Train: 0.7167, Val: 0.4460, Test: 0.4700\n",
            "Epoch: 011, Train: 0.8000, Val: 0.4760, Test: 0.4750\n",
            "Epoch: 012, Train: 0.8083, Val: 0.4980, Test: 0.4900\n",
            "Epoch: 013, Train: 0.8083, Val: 0.5680, Test: 0.5500\n",
            "Epoch: 014, Train: 0.8333, Val: 0.6220, Test: 0.5790\n",
            "Epoch: 015, Train: 0.8250, Val: 0.6220, Test: 0.5790\n",
            "Epoch: 016, Train: 0.8417, Val: 0.6220, Test: 0.5790\n",
            "Epoch: 017, Train: 0.8583, Val: 0.6520, Test: 0.6480\n",
            "Epoch: 018, Train: 0.8833, Val: 0.6680, Test: 0.6770\n",
            "Epoch: 019, Train: 0.9000, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 020, Train: 0.8917, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 021, Train: 0.9083, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 022, Train: 0.8917, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 023, Train: 0.9083, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 024, Train: 0.9083, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 025, Train: 0.9250, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 026, Train: 0.9250, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 027, Train: 0.9333, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 028, Train: 0.9333, Val: 0.6740, Test: 0.6970\n",
            "Epoch: 029, Train: 0.9500, Val: 0.6780, Test: 0.6640\n",
            "Epoch: 030, Train: 0.9333, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 031, Train: 0.9500, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 032, Train: 0.9500, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 033, Train: 0.9500, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 034, Train: 0.9333, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 035, Train: 0.9500, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 036, Train: 0.9500, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 037, Train: 0.9417, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 038, Train: 0.9500, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 039, Train: 0.9417, Val: 0.6860, Test: 0.6680\n",
            "Epoch: 040, Train: 0.9583, Val: 0.6920, Test: 0.6910\n",
            "Epoch: 041, Train: 0.9583, Val: 0.6920, Test: 0.6910\n",
            "Epoch: 042, Train: 0.9500, Val: 0.6920, Test: 0.6910\n",
            "Epoch: 043, Train: 0.9667, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 044, Train: 0.9667, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 045, Train: 0.9667, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 046, Train: 0.9750, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 047, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 048, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 049, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 050, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 051, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 052, Train: 0.9750, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 053, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 054, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 055, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 056, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 057, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 058, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 059, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 060, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 061, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 062, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 063, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 064, Train: 0.9667, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 065, Train: 0.9583, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 066, Train: 0.9583, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 067, Train: 0.9750, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 068, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 069, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 070, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 071, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 072, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 073, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 074, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 075, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 076, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 077, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 078, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 079, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 080, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 081, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 082, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 083, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 084, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 085, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 086, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 087, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 088, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 089, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 090, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 091, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 092, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 093, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 094, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 095, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 096, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 097, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 098, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 099, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 100, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 101, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 102, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 103, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 104, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 105, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 106, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 107, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 108, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 109, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 110, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 111, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 112, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 113, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 114, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 115, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 116, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 117, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 118, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 119, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 120, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 121, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 122, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 123, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 124, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 125, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 126, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 127, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 128, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 129, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 130, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 131, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 132, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 133, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 134, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 135, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 136, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 137, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 138, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 139, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 140, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 141, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 142, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 143, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 144, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 145, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 146, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 147, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 148, Train: 0.9833, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 149, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 150, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 151, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 152, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 153, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 154, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 155, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 156, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 157, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 158, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 159, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 160, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 161, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 162, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 163, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 164, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 165, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 166, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 167, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 168, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 169, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 170, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 171, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 172, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 173, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 174, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 175, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 176, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 177, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 178, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 179, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 180, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 181, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 182, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 183, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 184, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 185, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 186, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 187, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 188, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 189, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 190, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 191, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 192, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 193, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 194, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 195, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 196, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 197, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 198, Train: 1.0000, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 199, Train: 0.9917, Val: 0.6960, Test: 0.6980\n",
            "Epoch: 200, Train: 0.9917, Val: 0.6960, Test: 0.6980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth 4"
      ],
      "metadata": {
        "id": "ntJraeT_ybey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = DeepGUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=13, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY0mQAWJIXm2",
        "outputId": "2422ca59-67a8-48cd-b46c-cdfd8c8d39af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.2000, Val: 0.1020, Test: 0.1020\n",
            "Epoch: 002, Train: 0.2417, Val: 0.2180, Test: 0.2080\n",
            "Epoch: 003, Train: 0.2333, Val: 0.2180, Test: 0.2080\n",
            "Epoch: 004, Train: 0.3583, Val: 0.3560, Test: 0.3270\n",
            "Epoch: 005, Train: 0.4000, Val: 0.3560, Test: 0.3270\n",
            "Epoch: 006, Train: 0.3917, Val: 0.3560, Test: 0.3270\n",
            "Epoch: 007, Train: 0.5667, Val: 0.3600, Test: 0.3460\n",
            "Epoch: 008, Train: 0.4417, Val: 0.3600, Test: 0.3460\n",
            "Epoch: 009, Train: 0.4583, Val: 0.3600, Test: 0.3460\n",
            "Epoch: 010, Train: 0.5750, Val: 0.3600, Test: 0.3460\n",
            "Epoch: 011, Train: 0.6583, Val: 0.3600, Test: 0.3460\n",
            "Epoch: 012, Train: 0.6917, Val: 0.3600, Test: 0.3460\n",
            "Epoch: 013, Train: 0.7000, Val: 0.3860, Test: 0.3720\n",
            "Epoch: 014, Train: 0.7583, Val: 0.4320, Test: 0.4210\n",
            "Epoch: 015, Train: 0.8167, Val: 0.5180, Test: 0.4920\n",
            "Epoch: 016, Train: 0.8750, Val: 0.5680, Test: 0.5420\n",
            "Epoch: 017, Train: 0.8917, Val: 0.5840, Test: 0.5730\n",
            "Epoch: 018, Train: 0.9000, Val: 0.5840, Test: 0.5730\n",
            "Epoch: 019, Train: 0.9167, Val: 0.5880, Test: 0.5810\n",
            "Epoch: 020, Train: 0.9083, Val: 0.6260, Test: 0.6130\n",
            "Epoch: 021, Train: 0.9083, Val: 0.6480, Test: 0.6250\n",
            "Epoch: 022, Train: 0.9167, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 023, Train: 0.8750, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 024, Train: 0.9083, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 025, Train: 0.9500, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 026, Train: 0.9583, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 027, Train: 0.9417, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 028, Train: 0.9417, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 029, Train: 0.9333, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 030, Train: 0.9333, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 031, Train: 0.9333, Val: 0.6580, Test: 0.6520\n",
            "Epoch: 032, Train: 0.9333, Val: 0.6760, Test: 0.6770\n",
            "Epoch: 033, Train: 0.9333, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 034, Train: 0.9417, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 035, Train: 0.9417, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 036, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 037, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 038, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 039, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 040, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 041, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 042, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 043, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 044, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 045, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 046, Train: 0.9500, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 047, Train: 0.9667, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 048, Train: 0.9833, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 049, Train: 0.9750, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 050, Train: 0.9833, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 051, Train: 0.9750, Val: 0.6820, Test: 0.6930\n",
            "Epoch: 052, Train: 0.9667, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 053, Train: 0.9750, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 054, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 055, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 056, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 057, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 058, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 059, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 060, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 061, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 062, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 063, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 064, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 065, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 066, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 067, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 068, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 069, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 070, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 071, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 072, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 073, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 074, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 075, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 076, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 077, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 078, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 079, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 080, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 081, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 082, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 083, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 084, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 085, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 086, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 087, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 088, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 089, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 090, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 091, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 092, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 093, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 094, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 095, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 096, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 097, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 098, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 099, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 100, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 101, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 102, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 103, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 104, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 105, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 106, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 107, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 108, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 109, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 110, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 111, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 112, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 113, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 114, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 115, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 116, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 117, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 118, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 119, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 120, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 121, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 122, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 123, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 124, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 125, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 126, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 127, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 128, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 129, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 130, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 131, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 132, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 133, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 134, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 135, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 136, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 137, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 138, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 139, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 140, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 141, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 142, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 143, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 144, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 145, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 146, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 147, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 148, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 149, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 150, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 151, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 152, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 153, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 154, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 155, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 156, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 157, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 158, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 159, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 160, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 161, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 162, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 163, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 164, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 165, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 166, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 167, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 168, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 169, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 170, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 171, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 172, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 173, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 174, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 175, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 176, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 177, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 178, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 179, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 180, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 181, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 182, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 183, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 184, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 185, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 186, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 187, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 188, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 189, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 190, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 191, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 192, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 193, Train: 1.0000, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 194, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 195, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 196, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 197, Train: 0.9833, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 198, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 199, Train: 0.9917, Val: 0.6960, Test: 0.6970\n",
            "Epoch: 200, Train: 1.0000, Val: 0.6960, Test: 0.6970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth 20"
      ],
      "metadata": {
        "id": "DLXQGxyfyeho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = DeepGUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=20, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkS7WPUjIguH",
        "outputId": "2cf0f1d1-c720-4702-bdba-f175435b9007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.1917, Val: 0.2120, Test: 0.1870\n",
            "Epoch: 002, Train: 0.1833, Val: 0.2120, Test: 0.1870\n",
            "Epoch: 003, Train: 0.1750, Val: 0.2120, Test: 0.1870\n",
            "Epoch: 004, Train: 0.2417, Val: 0.2120, Test: 0.1870\n",
            "Epoch: 005, Train: 0.4000, Val: 0.2820, Test: 0.3250\n",
            "Epoch: 006, Train: 0.4000, Val: 0.3320, Test: 0.3450\n",
            "Epoch: 007, Train: 0.5250, Val: 0.3480, Test: 0.3430\n",
            "Epoch: 008, Train: 0.4833, Val: 0.3480, Test: 0.3430\n",
            "Epoch: 009, Train: 0.4750, Val: 0.3480, Test: 0.3430\n",
            "Epoch: 010, Train: 0.4667, Val: 0.3480, Test: 0.3430\n",
            "Epoch: 011, Train: 0.5750, Val: 0.3480, Test: 0.3430\n",
            "Epoch: 012, Train: 0.6167, Val: 0.4180, Test: 0.4070\n",
            "Epoch: 013, Train: 0.7083, Val: 0.4940, Test: 0.4940\n",
            "Epoch: 014, Train: 0.8167, Val: 0.5720, Test: 0.5530\n",
            "Epoch: 015, Train: 0.8500, Val: 0.5840, Test: 0.5560\n",
            "Epoch: 016, Train: 0.8667, Val: 0.6120, Test: 0.5920\n",
            "Epoch: 017, Train: 0.9167, Val: 0.6180, Test: 0.6170\n",
            "Epoch: 018, Train: 0.9167, Val: 0.6180, Test: 0.6170\n",
            "Epoch: 019, Train: 0.9250, Val: 0.6180, Test: 0.6170\n",
            "Epoch: 020, Train: 0.9250, Val: 0.6180, Test: 0.6170\n",
            "Epoch: 021, Train: 0.9333, Val: 0.6560, Test: 0.6710\n",
            "Epoch: 022, Train: 0.9333, Val: 0.6580, Test: 0.6660\n",
            "Epoch: 023, Train: 0.9333, Val: 0.6600, Test: 0.6760\n",
            "Epoch: 024, Train: 0.9250, Val: 0.6600, Test: 0.6760\n",
            "Epoch: 025, Train: 0.9250, Val: 0.6680, Test: 0.6790\n",
            "Epoch: 026, Train: 0.9250, Val: 0.6680, Test: 0.6790\n",
            "Epoch: 027, Train: 0.9333, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 028, Train: 0.9417, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 029, Train: 0.9500, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 030, Train: 0.9500, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 031, Train: 0.9500, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 032, Train: 0.9500, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 033, Train: 0.9583, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 034, Train: 0.9583, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 035, Train: 0.9583, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 036, Train: 0.9500, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 037, Train: 0.9583, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 038, Train: 0.9500, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 039, Train: 0.9500, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 040, Train: 0.9500, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 041, Train: 0.9500, Val: 0.6840, Test: 0.6780\n",
            "Epoch: 042, Train: 0.9583, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 043, Train: 0.9750, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 044, Train: 0.9667, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 045, Train: 0.9500, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 046, Train: 0.9583, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 047, Train: 0.9750, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 048, Train: 0.9750, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 049, Train: 0.9750, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 050, Train: 0.9667, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 051, Train: 0.9667, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 052, Train: 0.9667, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 053, Train: 0.9750, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 054, Train: 0.9833, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 055, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 056, Train: 0.9833, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 057, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 058, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 059, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 060, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 061, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 062, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 063, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 064, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 065, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 066, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 067, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 068, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 069, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 070, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 071, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 072, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 073, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 074, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 075, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 076, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 077, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 078, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 079, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 080, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 081, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 082, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 083, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 084, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 085, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 086, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 087, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 088, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 089, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 090, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 091, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 092, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 093, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 094, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 095, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 096, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 097, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 098, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 099, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 100, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 101, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 102, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 103, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 104, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 105, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 106, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 107, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 108, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 109, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 110, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 111, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 112, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 113, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 114, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 115, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 116, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 117, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 118, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 119, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 120, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 121, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 122, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 123, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 124, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 125, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 126, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 127, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 128, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 129, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 130, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 131, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 132, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 133, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 134, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 135, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 136, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 137, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 138, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 139, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 140, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 141, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 142, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 143, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 144, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 145, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 146, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 147, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 148, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 149, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 150, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 151, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 152, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 153, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 154, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 155, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 156, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 157, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 158, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 159, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 160, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 161, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 162, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 163, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 164, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 165, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 166, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 167, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 168, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 169, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 170, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 171, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 172, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 173, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 174, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 175, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 176, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 177, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 178, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 179, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 180, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 181, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 182, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 183, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 184, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 185, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 186, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 187, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 188, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 189, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 190, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 191, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 192, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 193, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 194, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 195, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 196, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 197, Train: 0.9917, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 198, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 199, Train: 1.0000, Val: 0.6860, Test: 0.6900\n",
            "Epoch: 200, Train: 1.0000, Val: 0.6860, Test: 0.6900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth 50"
      ],
      "metadata": {
        "id": "HTL9l0anyhaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = DeepGUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=50, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS3cU7sbIpj-",
        "outputId": "227b7174-b187-4393-9fa4-3b8f354c9263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.2000, Val: 0.1220, Test: 0.1230\n",
            "Epoch: 002, Train: 0.2917, Val: 0.1640, Test: 0.1660\n",
            "Epoch: 003, Train: 0.3167, Val: 0.2280, Test: 0.2440\n",
            "Epoch: 004, Train: 0.3917, Val: 0.3160, Test: 0.3030\n",
            "Epoch: 005, Train: 0.4250, Val: 0.3220, Test: 0.3020\n",
            "Epoch: 006, Train: 0.4500, Val: 0.3340, Test: 0.3220\n",
            "Epoch: 007, Train: 0.5333, Val: 0.3920, Test: 0.3880\n",
            "Epoch: 008, Train: 0.5667, Val: 0.4340, Test: 0.4150\n",
            "Epoch: 009, Train: 0.5833, Val: 0.4420, Test: 0.4020\n",
            "Epoch: 010, Train: 0.6750, Val: 0.4420, Test: 0.4020\n",
            "Epoch: 011, Train: 0.7667, Val: 0.4420, Test: 0.4020\n",
            "Epoch: 012, Train: 0.8083, Val: 0.4820, Test: 0.4730\n",
            "Epoch: 013, Train: 0.8167, Val: 0.4820, Test: 0.4730\n",
            "Epoch: 014, Train: 0.8583, Val: 0.4820, Test: 0.4730\n",
            "Epoch: 015, Train: 0.8667, Val: 0.5100, Test: 0.5150\n",
            "Epoch: 016, Train: 0.9000, Val: 0.5280, Test: 0.5540\n",
            "Epoch: 017, Train: 0.9083, Val: 0.5720, Test: 0.5800\n",
            "Epoch: 018, Train: 0.9167, Val: 0.6220, Test: 0.6120\n",
            "Epoch: 019, Train: 0.8000, Val: 0.6220, Test: 0.6120\n",
            "Epoch: 020, Train: 0.8500, Val: 0.6220, Test: 0.6120\n",
            "Epoch: 021, Train: 0.8833, Val: 0.6220, Test: 0.6120\n",
            "Epoch: 022, Train: 0.9167, Val: 0.6220, Test: 0.6120\n",
            "Epoch: 023, Train: 0.9333, Val: 0.6220, Test: 0.6120\n",
            "Epoch: 024, Train: 0.9417, Val: 0.6520, Test: 0.6380\n",
            "Epoch: 025, Train: 0.9667, Val: 0.6560, Test: 0.6690\n",
            "Epoch: 026, Train: 0.9583, Val: 0.6560, Test: 0.6690\n",
            "Epoch: 027, Train: 0.9333, Val: 0.6560, Test: 0.6690\n",
            "Epoch: 028, Train: 0.9333, Val: 0.6560, Test: 0.6690\n",
            "Epoch: 029, Train: 0.9333, Val: 0.6560, Test: 0.6690\n",
            "Epoch: 030, Train: 0.9417, Val: 0.6560, Test: 0.6690\n",
            "Epoch: 031, Train: 0.9417, Val: 0.6560, Test: 0.6690\n",
            "Epoch: 032, Train: 0.9250, Val: 0.6560, Test: 0.6690\n",
            "Epoch: 033, Train: 0.9333, Val: 0.6560, Test: 0.6690\n",
            "Epoch: 034, Train: 0.9417, Val: 0.6780, Test: 0.6660\n",
            "Epoch: 035, Train: 0.9500, Val: 0.6840, Test: 0.6750\n",
            "Epoch: 036, Train: 0.9417, Val: 0.6940, Test: 0.6900\n",
            "Epoch: 037, Train: 0.9583, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 038, Train: 0.9583, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 039, Train: 0.9667, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 040, Train: 0.9583, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 041, Train: 0.9583, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 042, Train: 0.9583, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 043, Train: 0.9500, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 044, Train: 0.9583, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 045, Train: 0.9667, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 046, Train: 0.9667, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 047, Train: 0.9833, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 048, Train: 0.9833, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 049, Train: 0.9833, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 050, Train: 0.9833, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 051, Train: 0.9833, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 052, Train: 0.9833, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 053, Train: 0.9833, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 054, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 055, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 056, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 057, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 058, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 059, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 060, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 061, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 062, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 063, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 064, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 065, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 066, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 067, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 068, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 069, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 070, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 071, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 072, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 073, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 074, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 075, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 076, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 077, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 078, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 079, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 080, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 081, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 082, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 083, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 084, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 085, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 086, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 087, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 088, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 089, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 090, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 091, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 092, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 093, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 094, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 095, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 096, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 097, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 098, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 099, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 100, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 101, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 102, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 103, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 104, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 105, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 106, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 107, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 108, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 109, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 110, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 111, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 112, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 113, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 114, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 115, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 116, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 117, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 118, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 119, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 120, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 121, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 122, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 123, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 124, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 125, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 126, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 127, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 128, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 129, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 130, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 131, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 132, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 133, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 134, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 135, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 136, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 137, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 138, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 139, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 140, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 141, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 142, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 143, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 144, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 145, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 146, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 147, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 148, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 149, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 150, Train: 0.9833, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 151, Train: 0.9833, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 152, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 153, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 154, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 155, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 156, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 157, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 158, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 159, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 160, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 161, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 162, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 163, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 164, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 165, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 166, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 167, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 168, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 169, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 170, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 171, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 172, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 173, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 174, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 175, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 176, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 177, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 178, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 179, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 180, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 181, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 182, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 183, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 184, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 185, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 186, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 187, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 188, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 189, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 190, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 191, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 192, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 193, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 194, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 195, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 196, Train: 1.0000, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 197, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 198, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 199, Train: 0.9917, Val: 0.6960, Test: 0.6890\n",
            "Epoch: 200, Train: 0.9917, Val: 0.6960, Test: 0.6890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth 100"
      ],
      "metadata": {
        "id": "bXA2IZNdylro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = DeepGUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=100, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6Z7K8Quee7b",
        "outputId": "9073146c-3cab-4a82-bc02-173101cb4674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.2167, Val: 0.1860, Test: 0.2300\n",
            "Epoch: 002, Train: 0.2500, Val: 0.2820, Test: 0.2510\n",
            "Epoch: 003, Train: 0.3583, Val: 0.2840, Test: 0.2440\n",
            "Epoch: 004, Train: 0.4417, Val: 0.2840, Test: 0.2440\n",
            "Epoch: 005, Train: 0.4250, Val: 0.2840, Test: 0.2440\n",
            "Epoch: 006, Train: 0.5333, Val: 0.3240, Test: 0.3310\n",
            "Epoch: 007, Train: 0.5833, Val: 0.3880, Test: 0.4030\n",
            "Epoch: 008, Train: 0.5500, Val: 0.3920, Test: 0.4130\n",
            "Epoch: 009, Train: 0.6000, Val: 0.4100, Test: 0.4280\n",
            "Epoch: 010, Train: 0.6333, Val: 0.4120, Test: 0.4400\n",
            "Epoch: 011, Train: 0.7250, Val: 0.4640, Test: 0.4780\n",
            "Epoch: 012, Train: 0.8417, Val: 0.5340, Test: 0.5570\n",
            "Epoch: 013, Train: 0.9000, Val: 0.5660, Test: 0.5730\n",
            "Epoch: 014, Train: 0.9250, Val: 0.5800, Test: 0.5830\n",
            "Epoch: 015, Train: 0.9250, Val: 0.5800, Test: 0.5830\n",
            "Epoch: 016, Train: 0.9333, Val: 0.5820, Test: 0.5690\n",
            "Epoch: 017, Train: 0.9250, Val: 0.5940, Test: 0.5740\n",
            "Epoch: 018, Train: 0.9250, Val: 0.6040, Test: 0.6120\n",
            "Epoch: 019, Train: 0.9250, Val: 0.6300, Test: 0.6360\n",
            "Epoch: 020, Train: 0.9250, Val: 0.6580, Test: 0.6660\n",
            "Epoch: 021, Train: 0.9167, Val: 0.6720, Test: 0.6880\n",
            "Epoch: 022, Train: 0.9167, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 023, Train: 0.9250, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 024, Train: 0.9333, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 025, Train: 0.9417, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 026, Train: 0.9417, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 027, Train: 0.9417, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 028, Train: 0.9417, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 029, Train: 0.9333, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 030, Train: 0.9333, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 031, Train: 0.9333, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 032, Train: 0.9417, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 033, Train: 0.9500, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 034, Train: 0.9583, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 035, Train: 0.9667, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 036, Train: 0.9667, Val: 0.6900, Test: 0.6860\n",
            "Epoch: 037, Train: 0.9667, Val: 0.6920, Test: 0.6790\n",
            "Epoch: 038, Train: 0.9667, Val: 0.6980, Test: 0.6860\n",
            "Epoch: 039, Train: 0.9750, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 040, Train: 0.9750, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 041, Train: 0.9750, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 042, Train: 0.9667, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 043, Train: 0.9750, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 044, Train: 0.9750, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 045, Train: 0.9750, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 046, Train: 0.9667, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 047, Train: 0.9500, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 048, Train: 0.9667, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 049, Train: 0.9667, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 050, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 051, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 052, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 053, Train: 0.9750, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 054, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 055, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 056, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 057, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 058, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 059, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 060, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 061, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 062, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 063, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 064, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 065, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 066, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 067, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 068, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 069, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 070, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 071, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 072, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 073, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 074, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 075, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 076, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 077, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 078, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 079, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 080, Train: 0.9750, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 081, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 082, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 083, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 084, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 085, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 086, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 087, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 088, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 089, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 090, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 091, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 092, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 093, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 094, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 095, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 096, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 097, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 098, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 099, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 100, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 101, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 102, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 103, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 104, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 105, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 106, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 107, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 108, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 109, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 110, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 111, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 112, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 113, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 114, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 115, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 116, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 117, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 118, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 119, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 120, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 121, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 122, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 123, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 124, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 125, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 126, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 127, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 128, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 129, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 130, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 131, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 132, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 133, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 134, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 135, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 136, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 137, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 138, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 139, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 140, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 141, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 142, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 143, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 144, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 145, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 146, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 147, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 148, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 149, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 150, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 151, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 152, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 153, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 154, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 155, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 156, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 157, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 158, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 159, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 160, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 161, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 162, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 163, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 164, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 165, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 166, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 167, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 168, Train: 1.0000, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 169, Train: 0.9917, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 170, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 171, Train: 0.9833, Val: 0.7020, Test: 0.6890\n",
            "Epoch: 172, Train: 0.9750, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 173, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 174, Train: 1.0000, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 175, Train: 1.0000, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 176, Train: 1.0000, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 177, Train: 1.0000, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 178, Train: 1.0000, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 179, Train: 1.0000, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 180, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 181, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 182, Train: 0.9833, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 183, Train: 0.9750, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 184, Train: 0.9750, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 185, Train: 0.9833, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 186, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 187, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 188, Train: 1.0000, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 189, Train: 1.0000, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 190, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 191, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 192, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 193, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 194, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 195, Train: 0.9917, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 196, Train: 0.9833, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 197, Train: 0.9750, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 198, Train: 0.9833, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 199, Train: 0.9833, Val: 0.7060, Test: 0.6970\n",
            "Epoch: 200, Train: 0.9917, Val: 0.7060, Test: 0.6970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth 500"
      ],
      "metadata": {
        "id": "iofeQb30yq7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = DeepGUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=500, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ljHriMQBfFgI",
        "outputId": "e8bc6bdc-d3b9-406d-eddd-189e36302097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.2250, Val: 0.1400, Test: 0.1580\n",
            "Epoch: 002, Train: 0.3250, Val: 0.3020, Test: 0.2910\n",
            "Epoch: 003, Train: 0.3500, Val: 0.3020, Test: 0.2910\n",
            "Epoch: 004, Train: 0.3833, Val: 0.3120, Test: 0.2800\n",
            "Epoch: 005, Train: 0.5333, Val: 0.3760, Test: 0.3760\n",
            "Epoch: 006, Train: 0.5417, Val: 0.3760, Test: 0.3760\n",
            "Epoch: 007, Train: 0.6250, Val: 0.4000, Test: 0.4180\n",
            "Epoch: 008, Train: 0.7417, Val: 0.4620, Test: 0.4830\n",
            "Epoch: 009, Train: 0.8667, Val: 0.5280, Test: 0.5540\n",
            "Epoch: 010, Train: 0.8917, Val: 0.5840, Test: 0.5920\n",
            "Epoch: 011, Train: 0.9167, Val: 0.5840, Test: 0.5920\n",
            "Epoch: 012, Train: 0.9083, Val: 0.5840, Test: 0.5920\n",
            "Epoch: 013, Train: 0.9167, Val: 0.5840, Test: 0.5920\n",
            "Epoch: 014, Train: 0.9250, Val: 0.5840, Test: 0.5920\n",
            "Epoch: 015, Train: 0.9417, Val: 0.5840, Test: 0.5920\n",
            "Epoch: 016, Train: 0.9583, Val: 0.5840, Test: 0.5920\n",
            "Epoch: 017, Train: 0.9500, Val: 0.5920, Test: 0.5910\n",
            "Epoch: 018, Train: 0.9417, Val: 0.6340, Test: 0.6190\n",
            "Epoch: 019, Train: 0.9417, Val: 0.6400, Test: 0.6360\n",
            "Epoch: 020, Train: 0.9333, Val: 0.6400, Test: 0.6360\n",
            "Epoch: 021, Train: 0.9417, Val: 0.6540, Test: 0.6660\n",
            "Epoch: 022, Train: 0.9333, Val: 0.6540, Test: 0.6660\n",
            "Epoch: 023, Train: 0.9333, Val: 0.6540, Test: 0.6660\n",
            "Epoch: 024, Train: 0.9417, Val: 0.6540, Test: 0.6660\n",
            "Epoch: 025, Train: 0.9333, Val: 0.6540, Test: 0.6660\n",
            "Epoch: 026, Train: 0.9333, Val: 0.6560, Test: 0.6600\n",
            "Epoch: 027, Train: 0.9333, Val: 0.6560, Test: 0.6600\n",
            "Epoch: 028, Train: 0.9417, Val: 0.6560, Test: 0.6600\n",
            "Epoch: 029, Train: 0.9417, Val: 0.6560, Test: 0.6600\n",
            "Epoch: 030, Train: 0.9417, Val: 0.6560, Test: 0.6600\n",
            "Epoch: 031, Train: 0.9417, Val: 0.6560, Test: 0.6600\n",
            "Epoch: 032, Train: 0.9583, Val: 0.6620, Test: 0.6600\n",
            "Epoch: 033, Train: 0.9667, Val: 0.6620, Test: 0.6600\n",
            "Epoch: 034, Train: 0.9750, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 035, Train: 0.9667, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 036, Train: 0.9583, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 037, Train: 0.9583, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 038, Train: 0.9583, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 039, Train: 0.9583, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 040, Train: 0.9750, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 041, Train: 0.9833, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 042, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 043, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 044, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 045, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 046, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 047, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 048, Train: 0.9833, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 049, Train: 0.9833, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 050, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 051, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 052, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 053, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 054, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 055, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 056, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 057, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 058, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 059, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 060, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 061, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 062, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 063, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 064, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 065, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 066, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 067, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 068, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 069, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 070, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 071, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 072, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 073, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 074, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 075, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 076, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 077, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 078, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 079, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 080, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 081, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 082, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 083, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 084, Train: 1.0000, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 085, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 086, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 087, Train: 0.9917, Val: 0.6800, Test: 0.6770\n",
            "Epoch: 088, Train: 0.9917, Val: 0.6920, Test: 0.6640\n",
            "Epoch: 089, Train: 1.0000, Val: 0.6920, Test: 0.6640\n",
            "Epoch: 090, Train: 1.0000, Val: 0.6920, Test: 0.6640\n",
            "Epoch: 091, Train: 1.0000, Val: 0.6920, Test: 0.6640\n",
            "Epoch: 092, Train: 1.0000, Val: 0.6920, Test: 0.6640\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-861914635.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m201\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-861914635.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-861914635.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.92\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1388771575.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 x, edge_index, edge_weight, batch)\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_convs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_edge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                     edge_index, edge_weight = gcn_norm(  # yapf: disable\n\u001b[0m\u001b[1;32m    242\u001b[0m                         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                         self.improved, self.add_self_loops, self.flow, x.dtype)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mflow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'source_to_target'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mdeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0medge_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "44v4eq7eytZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth 1000"
      ],
      "metadata": {
        "id": "ugKW4e7Oytpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pool_ratios = [2000 / data.num_nodes, 0.5]\n",
        "        self.unet = DeepGUNet(dataset.num_features, 32, dataset.num_classes,\n",
        "                              depth=1000, pool_ratios=pool_ratios)\n",
        "\n",
        "    def forward(self):\n",
        "        edge_index, _ = dropout_edge(data.edge_index, p=0.2,\n",
        "                                     force_undirected=True,\n",
        "                                     training=self.training)\n",
        "        x = F.dropout(data.x, p=0.92, training=self.training)\n",
        "\n",
        "        x = self.unet(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, data = Net().to(device), data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test():\n",
        "    model.eval()\n",
        "    out, accs = model(), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = out[mask].argmax(1)\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "best_val_acc = test_acc = 0\n",
        "for epoch in range(1, 201):\n",
        "    train()\n",
        "    train_acc, val_acc, tmp_test_acc = test()\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        test_acc = tmp_test_acc\n",
        "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, '\n",
        "          f'Val: {best_val_acc:.4f}, Test: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7it6YXvhWKG",
        "outputId": "70808aa1-e107-48fd-c07c-c4196c288c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.1750, Val: 0.2160, Test: 0.1920\n",
            "Epoch: 002, Train: 0.3083, Val: 0.2700, Test: 0.2490\n",
            "Epoch: 003, Train: 0.2333, Val: 0.2700, Test: 0.2490\n",
            "Epoch: 004, Train: 0.3250, Val: 0.2760, Test: 0.2630\n",
            "Epoch: 005, Train: 0.3333, Val: 0.2960, Test: 0.2800\n",
            "Epoch: 006, Train: 0.5083, Val: 0.3820, Test: 0.3610\n",
            "Epoch: 007, Train: 0.5250, Val: 0.3820, Test: 0.3610\n",
            "Epoch: 008, Train: 0.5250, Val: 0.3820, Test: 0.3610\n",
            "Epoch: 009, Train: 0.6083, Val: 0.3940, Test: 0.3930\n",
            "Epoch: 010, Train: 0.6583, Val: 0.4680, Test: 0.4540\n",
            "Epoch: 011, Train: 0.7333, Val: 0.4980, Test: 0.4770\n",
            "Epoch: 012, Train: 0.7917, Val: 0.5080, Test: 0.4980\n",
            "Epoch: 013, Train: 0.8667, Val: 0.5700, Test: 0.5720\n",
            "Epoch: 014, Train: 0.9000, Val: 0.6320, Test: 0.6350\n",
            "Epoch: 015, Train: 0.9000, Val: 0.6400, Test: 0.6380\n",
            "Epoch: 016, Train: 0.9083, Val: 0.6600, Test: 0.6480\n",
            "Epoch: 017, Train: 0.9167, Val: 0.6600, Test: 0.6480\n",
            "Epoch: 018, Train: 0.9250, Val: 0.6600, Test: 0.6480\n",
            "Epoch: 019, Train: 0.9250, Val: 0.6600, Test: 0.6480\n",
            "Epoch: 020, Train: 0.9250, Val: 0.6600, Test: 0.6480\n",
            "Epoch: 021, Train: 0.9333, Val: 0.6600, Test: 0.6480\n",
            "Epoch: 022, Train: 0.9333, Val: 0.6800, Test: 0.6630\n",
            "Epoch: 023, Train: 0.9417, Val: 0.6800, Test: 0.6630\n",
            "Epoch: 024, Train: 0.9500, Val: 0.6800, Test: 0.6630\n",
            "Epoch: 025, Train: 0.9500, Val: 0.6800, Test: 0.6630\n",
            "Epoch: 026, Train: 0.9417, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 027, Train: 0.9500, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 028, Train: 0.9417, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 029, Train: 0.9417, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 030, Train: 0.9500, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 031, Train: 0.9583, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 032, Train: 0.9583, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 033, Train: 0.9583, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 034, Train: 0.9667, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 035, Train: 0.9667, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 036, Train: 0.9667, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 037, Train: 0.9750, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 038, Train: 0.9750, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 039, Train: 0.9667, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 040, Train: 0.9583, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 041, Train: 0.9417, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 042, Train: 0.9500, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 043, Train: 0.9667, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 044, Train: 0.9667, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 045, Train: 0.9667, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 046, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 047, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 048, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 049, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 050, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 051, Train: 0.9750, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 052, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 053, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 054, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 055, Train: 0.9750, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 056, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 057, Train: 0.9833, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 058, Train: 0.9750, Val: 0.6960, Test: 0.6690\n",
            "Epoch: 059, Train: 0.9583, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 060, Train: 0.9667, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 061, Train: 0.9750, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 062, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 063, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 064, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 065, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 066, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 067, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 068, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 069, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 070, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 071, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 072, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 073, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 074, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 075, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 076, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 077, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 078, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 079, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 080, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 081, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 082, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 083, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 084, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 085, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 086, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 087, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 088, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 089, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 090, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 091, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 092, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 093, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 094, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 095, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 096, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 097, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 098, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 099, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 100, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 101, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 102, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 103, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 104, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 105, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 106, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 107, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 108, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 109, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 110, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 111, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 112, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 113, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 114, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 115, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 116, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 117, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 118, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 119, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 120, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 121, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 122, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 123, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 124, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 125, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 126, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 127, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 128, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 129, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 130, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 131, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 132, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 133, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 134, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 135, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 136, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 137, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 138, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 139, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 140, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 141, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 142, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 143, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 144, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 145, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 146, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 147, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 148, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 149, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 150, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 151, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 152, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 153, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 154, Train: 0.9750, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 155, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 156, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 157, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 158, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 159, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 160, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 161, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 162, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 163, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 164, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 165, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 166, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 167, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 168, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 169, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 170, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 171, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 172, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 173, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 174, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 175, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 176, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 177, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 178, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 179, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 180, Train: 0.9833, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 181, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 182, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 183, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 184, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 185, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 186, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 187, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 188, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 189, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 190, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 191, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 192, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 193, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 194, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 195, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 196, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 197, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 198, Train: 1.0000, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 199, Train: 0.9917, Val: 0.7000, Test: 0.6900\n",
            "Epoch: 200, Train: 0.9833, Val: 0.7000, Test: 0.6900\n"
          ]
        }
      ]
    }
  ]
}